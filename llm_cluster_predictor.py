import subprocess
import sys
import os
import json

# ì„¤ì¹˜ê°€ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ëŠ” ê²½ìš° ìë™ ì„¤ì¹˜
def install_required_packages(package, import_name=None):
    try:
        if import_name:
            __import__(import_name)
        else:
            __import__(package)
    except ImportError:
        print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ!")

required_packages = [
    ("scikit-learn", "sklearn"),
    ("tqdm", "tqdm"),
    ("openai", "openai"),
    ("pandas", "pandas"),
    ("numpy", "numpy")
]

for pkg, imp in required_packages:
    install_required_packages(pkg, imp)

from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from tqdm import tqdm
from openai import OpenAI
import pandas as pd
import numpy as np

# OpenAI API í‚¤ ì„¤ì •
USER_KEY = ""
client = OpenAI(api_key=USER_KEY)

# ë°ì´í„° ì¤€ë¹„
preprocessed_data = pd.read_csv('./ê´€ê´‘ë°ì´í„°.csv', encoding='cp949')

# ë³€ìˆ˜ êµ¬ë¶„
categorical_cols = ['country', 'gender', 'age',
                    'revisit_indicator', 'visit_local_indicator', 'planned_activity']

numerical_cols = [
    'stay_duration', 'accommodation_percent', 'food_percent', 'shopping_percent', 'food',
    'landscape', 'heritage', 'language', 'safety', 'budget',
    'accommodation', 'transport', 'navigation'
]
used_variables = categorical_cols + numerical_cols

for col in categorical_cols:
    preprocessed_data[col] = preprocessed_data[col].astype(str)
preprocessed_data_clean = preprocessed_data.dropna(subset=used_variables).copy()

# ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
# ìˆ˜ì¹˜í˜• íŒŒì´í”„ë¼ì¸: í‰ê·  ëŒ€ì²´ + ì •ê·œí™”
numeric_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', MinMaxScaler())
])

# ë²”ì£¼í˜• íŒŒì´í”„ë¼ì¸: ìµœë¹ˆê°’ ëŒ€ì²´ + ì›í•«ì¸ì½”ë”©
categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),  
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ('cat', categorical_pipeline, categorical_cols),
    ('num', numeric_pipeline, numerical_cols)
])

# í•™ìŠµ: ì „ì²˜ë¦¬ + PCA + í´ëŸ¬ìŠ¤í„°ë§
X_preprocessed = preprocessor.fit_transform(preprocessed_data_clean)
pca = PCA(n_components=3)
X_reduced = pca.fit_transform(X_preprocessed)

kmeans = KMeans(n_clusters=7, random_state=42)
preprocessed_data_clean['cluster'] = kmeans.fit_predict(X_reduced)

print("explained_variance_ratio:", pca.explained_variance_ratio_.sum())
print(f"Silhouette Score: {silhouette_score(X_reduced, preprocessed_data_clean['cluster']):.4f}")

# LLM ì§ˆì˜ = ë³€ìˆ˜ ë§¤í•‘ í•¨ìˆ˜
def load_text_file(filepath):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"[íŒŒì¼ ë¡œë”© ì‹¤íŒ¨] {filepath} - {e}")
        return ""

def query_llm_for_variables(user_query, use_prompt=True, use_fewshot=True):
    prompt_parts = []

    if use_prompt:
        with open("custom_prompt.txt", "r", encoding="utf-8") as f:
            custom_prompt = f.read()
            prompt_parts.append(custom_prompt)

    if use_fewshot:
        with open("custom_few_shot_learning.txt", "r", encoding="utf-8") as f:
            few_shot_examples = f.read()
            prompt_parts.append(few_shot_examples)

    full_prompt = "\n\n".join(prompt_parts)

    messages = [
        {"role": "system", "content": full_prompt},
        {"role": "user", "content": user_query}
    ]

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=messages
        )
        content = response.choices[0].message.content.strip()
        return json.loads(content)
    except Exception as e:
        print("[íŒŒì‹± ì‹¤íŒ¨]", e)
        return {}


def impute_with_user_subgroup(user_input_dict, df_base):
    known_info = {k: v for k, v in user_input_dict.items() if v is not None}
    filtered_df = df_base.copy()
    for key, val in known_info.items():
        if key in filtered_df.columns:
            filtered_df = filtered_df[filtered_df[key].astype(str) == str(val)]
    imputed = {}
    for var in used_variables:
        if user_input_dict.get(var) is not None:
            imputed[var] = user_input_dict[var]
        else:
            if var in numerical_cols:
                imputed[var] = filtered_df[var].mean() if not filtered_df.empty else df_base[var].mean()
            elif var in categorical_cols:
                mode_series = filtered_df[var].mode() if not filtered_df.empty else df_base[var].mode()
                imputed[var] = mode_series.iloc[0] if not mode_series.empty else None
    return imputed

# ì§ˆì˜ = ì˜ˆì¸¡ í•¨ìˆ˜
def predict_cluster_from_query(user_query):
    variable_dict = query_llm_for_variables(user_query, use_prompt=True, use_fewshot=True)
    
    # nullì´ ì•„ë‹Œ ê°’ë§Œ í•„í„°ë§í•˜ì—¬ ì¶œë ¥
    filtered_dict = {k: v for k, v in variable_dict.items() if v is not None}
    print("â®• LLM ì¶”ì¶œ ê²°ê³¼:", filtered_dict)

    # ê²°ì¸¡ ë³´ì™„
    completed_input = impute_with_user_subgroup(variable_dict, preprocessed_data_clean)
    df = pd.DataFrame([completed_input])

    for col in categorical_cols:
        df[col] = df[col].astype(str)
    for col in numerical_cols:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    try:
        X_processed = preprocessor.transform(df)
        X_pca = pca.transform(X_processed)
        cluster_label = kmeans.predict(X_pca)[0]
        return cluster_label
    except Exception as e:
        print("[ì˜ˆì¸¡ ì‹¤íŒ¨]", e)
        return None
    
# main block
if __name__ == "__main__":
    test_inputs = [
        "ë‚˜ëŠ” 50ëŒ€ ë‚¨ì„±ì´ê³ , ìì—° í’ê²½ì„ ì¢‹ì•„í•´ì„œ ì œì£¼ë„ì— 4ì¼ ì—¬í–‰í–ˆì–´ìš”",
        "ì €ëŠ” 20ëŒ€ ì—¬ì„±ì´ë©° ì‡¼í•‘ì„ ì¢‹ì•„í•´ìš”. ì„œìš¸ì—ì„œ 3ì¼ê°„ ë¨¸ë¬¼ë €ì–´ìš”",
        "ë‚˜ëŠ” 30ëŒ€ ë‚¨ìê³  í•œêµ­ ì „í†µë¬¸í™” ì²´í—˜ì´ ì¢‹ì•„ì„œ ì „ì£¼ì— ê°”ì–´ìš”. ì´ 5ì¼ ìˆì—ˆì–´ìš”",
        "ì €ëŠ” ë¯¸êµ­ì—ì„œ ì™”ê³ , ì²˜ìŒ ë°©ë¬¸í–ˆì–´ìš”. í•œêµ­ ìŒì‹ì— ê´€ì‹¬ì´ ë§ì•„ 6ì¼ê°„ ë¨¸ë¬¼ë €ì–´ìš”",
        "ì €ëŠ” ì¼ë³¸ ì—¬ì„±ì´ê³ , ë‘ ë²ˆì§¸ ë°©ë¬¸ì…ë‹ˆë‹¤. ìì—° í’ê²½ê³¼ ìœ ì ì§€ë¥¼ ë³´ê¸° ìœ„í•´ ê°•ì›ë„ì— 7ì¼ ë¨¸ë¬¼ë €ì–´ìš”"
    ]

    for i, user_input in enumerate(test_inputs, 1):
        cluster = predict_cluster_from_query(user_input)
        print(f"# ì‹¤í–‰ ì˜ˆì‹œ {i}")
        print(f"ì…ë ¥ ë¬¸ì¥: {user_input}")
        print(f"ì˜ˆì¸¡ëœ í´ëŸ¬ìŠ¤í„°: {cluster}\n")