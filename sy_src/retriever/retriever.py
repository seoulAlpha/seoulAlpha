import json
import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# Hugging Face ìºì‹œ ê²½ë¡œ ì„¤ì • (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
os.environ['HF_HOME'] = 'D:/huggingface_cache'

# --- ì„¤ì • ---
# ì„ë² ë”© ëª¨ë¸
MODEL_NAME = 'jhgan/ko-sbert-nli'
# ë‹µë³€ ìƒì„± LLM ëª¨ë¸
LLM_MODEL_NAME = 'gpt-4o-mini'

# íŒŒì¼ ê²½ë¡œ
OUTPUT_DIR = 'data/faiss/faiss_merged_output'
INDEX_FILE = f'{OUTPUT_DIR}/merged.index'
METADATA_FILE = f'{OUTPUT_DIR}/merged_metadata.jsonl'

# ê²€ìƒ‰í•  ê²°ê³¼ì˜ ìˆ˜
TOP_K = 10

import os
from dotenv import load_dotenv

# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ë¶€ë¶„ì—ì„œ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# --- ì„¤ì • ë ---


def load_resources():
    """
    ê²€ìƒ‰ì— í•„ìš”í•œ ëª¨ë¸, ì¸ë±ìŠ¤, ë©”íƒ€ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    print("1. ë¦¬ì†ŒìŠ¤ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤...")
    
    print(f"  - ì„ë² ë”© ëª¨ë¸({MODEL_NAME}) ë¡œë”© ì¤‘...")
    model = SentenceTransformer(MODEL_NAME)
    
    print(f"  - FAISS ì¸ë±ìŠ¤({INDEX_FILE}) ë¡œë”© ì¤‘...")
    index = faiss.read_index(INDEX_FILE)
    
    print(f"  - ë©”íƒ€ë°ì´í„°({METADATA_FILE}) ë¡œë”© ë° ë§¤í•‘ ì¤‘...")
    metadata_map = {}
    with open(METADATA_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            meta = json.loads(line)
            metadata_map[meta['vector_id']] = meta
            
    print("âœ… ë¦¬ì†ŒìŠ¤ ë¡œë”© ì™„ë£Œ!")
    return model, index, metadata_map


def retrieve_places(query, model, index, metadata_map, k):
    """
    ì£¼ì–´ì§„ ì¿¼ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ì¥ì†Œ Kê°œë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜
    """
    print("\n2. ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤...")
    query_vector = model.encode([query])
    
    print(f"\n3. FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬ë„ ë†’ì€ Top {k}ê°œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
    distances, ids = index.search(query_vector.astype('float32'), k)
    
    retrieved_ids = ids[0]
    
    print("\n4. ê²€ìƒ‰ëœ IDë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤...")
    results = []
    for vector_id in retrieved_ids:
        if vector_id in metadata_map:
            results.append(metadata_map[vector_id])
        else:
            print(f"  - âš ï¸ ê²½ê³ : ID {vector_id}ì— í•´ë‹¹í•˜ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return results


# ====================================================
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼ LLM ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# ====================================================

def generate_answer_with_llm(query, retrieved_places):
    """
    ê²€ìƒ‰ëœ ì¥ì†Œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì„ ì‚¬ìš©í•´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print("\n5. LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤...")

    # 1. LLMì—ê²Œ ì „ë‹¬í•  ì •ë³´(Context)ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    context = ""
    for i, place in enumerate(retrieved_places[:5]): # ë„ˆë¬´ ë§ì€ ì •ë³´ë¥¼ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
        context += f"--- ì¥ì†Œ ì •ë³´ {i+1} ---\n"
        context += f"ì´ë¦„: {place.get('name', 'ì •ë³´ ì—†ìŒ')}\n"
        context += f"AI ìš”ì•½: {place.get('ai_summary', 'ì •ë³´ ì—†ìŒ')}\n"
        processed_sentences = place.get('processed_sentences', [])
        context += "ì£¼ìš” íŠ¹ì§• ë° í›„ê¸°:\n"
        for sentence in processed_sentences:
            context += f"- {sentence}\n"
        context += "\n"

    # 2. LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    system_prompt = "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ìœ ìš©í•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
    
    user_prompt = f"""
    ì•„ë˜ 'ì¥ì†Œ ì •ë³´'ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•´ ì£¼ì„¸ìš”.

    [ì§€ì‹œì‚¬í•­]
    1. ê²€ìƒ‰ëœ ì¥ì†Œ ì¤‘ì—ì„œ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ 2~3ê³³ì„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    2. ê° ì¥ì†Œë¥¼ ì¶”ì²œí•˜ëŠ” ì´ìœ ë¥¼ 'AI ìš”ì•½'ê³¼ 'ì£¼ìš” íŠ¹ì§• ë° í›„ê¸°'ë¥¼ ê·¼ê±°ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
    3. 'processed_sentences'ì— ìˆëŠ” ì‹¤ì œ í›„ê¸°ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ë©´ ì‹ ë¢°ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    4. ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

    --- ì¥ì†Œ ì •ë³´ ---
    {context}
    --- ì‚¬ìš©ìì˜ ì§ˆë¬¸ ---
    {query}
    """

    try:
        # 3. OpenAI API í˜¸ì¶œ
        client = OpenAI() # í™˜ê²½ë³€ìˆ˜(OPENAI_API_KEY)ë¥¼ ìë™ìœ¼ë¡œ ì½ìŒ
        response = client.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âŒ LLM ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"


if __name__ == '__main__':
    # 1~4ë‹¨ê³„: ë¦¬ì†ŒìŠ¤ ë¡œë”© ë° ì¥ì†Œ ê²€ìƒ‰
    embedding_model, faiss_index, meta_map = load_resources()

    # 2. í•œêµ­ì–´ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
    queries = [
        "ì „í†µì ì¸ í•œêµ­ì˜ ë¯¸ë¥¼ ëŠë‚„ ìˆ˜ ìˆëŠ” ì¥ì†Œë¥¼ ì°¾ê³  ìˆì–´ìš”. ê³ ê¶ì´ë‚˜ ë¯¼ì†ì´Œ ê°™ì€ ê³³ì´ë©´ ì¢‹ê² ì–´ìš”.",
        "ìš”ì¦˜ ì¸ìŠ¤íƒ€ê·¸ë¨ì—ì„œ ìœ í–‰í•˜ëŠ” í™í•œ ì¹´í˜ ì¢€ ì¶”ì²œí•´ ì¤„ë˜?",
        "ì•„ë¦„ë‹¤ìš´ ìì—°ì„ ì¦ê¸°ê³  ì‹¶ì–´ìš”. ìœ ëª…í•œ ì‚°ì´ë‚˜ í•´ë³€ì´ ìˆë‚˜ìš”?",
        "ì „í˜•ì ì¸ ê´€ê´‘ì§€ ë§ê³ , ì¢€ ë…íŠ¹í•˜ê³  íŠ¹ë³„í•œ ê²½í—˜ì„ í•  ìˆ˜ ìˆëŠ” ê³³ì€ ì–´ë””ì¼ê¹Œìš”?",
        "ì»¤í”¼ í•œì”í•˜ë©´ì„œ ì•¼ê²½ ê°ìƒí•˜ê¸° ì¢‹ì€ ì¥ì†Œ ì¶”ì²œí•´ ì¤˜."
    ]

    # 3. ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    for user_query in queries:
        print("\n" + "#"*70)
        print(f"ğŸ—£ï¸  NEW QUERY: {user_query}")
        print("#"*70)

        # 1ë‹¨ê³„: ì¥ì†Œ ê²€ìƒ‰
        top_places = retrieve_places(
            query=user_query,
            model=embedding_model,
            index=faiss_index,
            metadata_map=meta_map,
            k=TOP_K
        )

        # ê²€ìƒ‰ëœ ì¥ì†Œ ëª©ë¡ ê°„ëµíˆ ì¶œë ¥
        print("\n" + "="*50)
        print(f"âœ… ê²€ìƒ‰ëœ Top {len(top_places)} ì¥ì†Œ ëª©ë¡ (ì˜ë¯¸ìˆœ)")
        print("="*50)
        for i, place in enumerate(top_places):
            name = place.get('name', 'N/A')
            address = place.get('address', 'N/A')
            print(f"  RANK {i+1}: {name} | {address}")
        print("="*50)

        # 2ë‹¨ê³„: LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        llm_answer = generate_answer_with_llm(user_query, top_places)
        
        # ìµœì¢… ë‹µë³€ ì¶œë ¥
        print("\n" + "="*50)
        print(f"ğŸ¤– LLM ì¶”ì²œ ìš”ì•½: '{user_query}'")
        print("="*50)
        print(llm_answer)
        print("="*50 + "\n\n")