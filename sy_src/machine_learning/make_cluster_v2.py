import subprocess
import sys
import os
import json
from dotenv import load_dotenv
import joblib  # ëª¨ë¸ ì €ì¥ì„ ìœ„í•´ import

# --- ì´ˆê¸° ì„¤ì • ---
# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
USER_KEY = os.getenv("API_KEY")

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜ í•¨ìˆ˜
def install_required_packages(package, import_name=None):
    try:
        if import_name:
            __import__(import_name)
        else:
            __import__(package)
    except ImportError:
        print(f"ğŸ“¦ {package} ì„¤ì¹˜ ì¤‘...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
        print(f"âœ… {package} ì„¤ì¹˜ ì™„ë£Œ!")

# ì„¤ì¹˜í•  íŒ¨í‚¤ì§€ ëª©ë¡
required_packages = [
    ("scikit-learn", "sklearn"),
    ("tqdm", "tqdm"),
    ("openai", "openai"),
    ("pandas", "pandas"),
    ("numpy", "numpy"),
    ("python-dotenv", "dotenv")
]

for pkg, imp in required_packages:
    install_required_packages(pkg, imp)

# ë¼ì´ë¸ŒëŸ¬ë¦¬ import
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
import pandas as pd
import numpy as np

# --- ë°ì´í„° ë° ë³€ìˆ˜ ì •ì˜ ---
# ë°ì´í„° ì¤€ë¹„

def main():
    try:
        preprocessed_data = pd.read_csv('./data/ê´€ê´‘ë°ì´í„°.csv', encoding='cp949')
    except FileNotFoundError:
        print("'ê´€ê´‘ë°ì´í„°.csv' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ìœ„ì¹˜ì— íŒŒì¼ì„ ë†“ì•„ì£¼ì„¸ìš”.")
        sys.exit()

    # ë³€ìˆ˜ êµ¬ë¶„
    categorical_cols = ['country', 'gender', 'age', 'revisit_indicator', 'visit_local_indicator', 'planned_activity']
    numerical_cols = [
        'stay_duration', 'accommodation_percent', 'food_percent', 'shopping_percent', 'food',
        'landscape', 'heritage', 'language', 'safety', 'budget', 'accommodation', 'transport', 'navigation'
    ]
    used_variables = categorical_cols + numerical_cols

    # ë°ì´í„° íƒ€ì… ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    for col in categorical_cols:
        preprocessed_data[col] = preprocessed_data[col].astype(str)
    preprocessed_data_clean = preprocessed_data.dropna(subset=used_variables).copy()

    # --- ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜ ---
    # ìˆ˜ì¹˜í˜• íŒŒì´í”„ë¼ì¸: í‰ê·  ëŒ€ì²´ + ì •ê·œí™”
    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', MinMaxScaler())
    ])

    # ë²”ì£¼í˜• íŒŒì´í”„ë¼ì¸: ìµœë¹ˆê°’ ëŒ€ì²´ + ì›í•«ì¸ì½”ë”©
    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])

    # ì „ì²˜ë¦¬ê¸° ê²°í•©
    preprocessor = ColumnTransformer(transformers=[
        ('cat', categorical_pipeline, categorical_cols),
        ('num', numeric_pipeline, numerical_cols)
    ])

    # --- ëª¨ë¸ í•™ìŠµ ---
    print("ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 1. ì „ì²˜ë¦¬ê¸°(preprocessor) í•™ìŠµ
    X_preprocessed = preprocessor.fit_transform(preprocessed_data_clean)

    # 2. PCA ëª¨ë¸ í•™ìŠµ
    pca = PCA(n_components=3)
    X_reduced = pca.fit_transform(X_preprocessed)

    # 3. K-Means ëª¨ë¸ í•™ìŠµ
    kmeans = KMeans(n_clusters=7, random_state=42)
    preprocessed_data_clean['cluster'] = kmeans.fit_predict(X_reduced)

    print("explained_variance_ratio:", pca.explained_variance_ratio_.sum())
    print(f"Silhouette Score: {silhouette_score(X_reduced, preprocessed_data_clean['cluster']):.4f}")

    # --- í•™ìŠµëœ ëª¨ë¸ ë° ë°ì´í„° ì €ì¥ ---
    # ì €ì¥í•  í´ë” ìƒì„±
    if not os.path.exists('./models'):
        os.makedirs('./models')


    joblib.dump(preprocessor, './models/preprocessor.joblib')
    joblib.dump(pca, './models/pca.joblib')
    joblib.dump(kmeans, './models/kmeans.joblib')
    # ì˜ˆì¸¡ ì‹œ ê²°ì¸¡ì¹˜ ë³´ì™„ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ë„ í•¨ê»˜ ì €ì¥
    preprocessed_data_clean.to_csv('./models/imputation_base_data.csv', index=False, encoding='utf-8-sig')

    # ==================== ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ ë° ê°€ì¤‘ì¹˜ íŒŒì¼ ìƒì„± ====================
    print("\në¶„ë¥˜ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ")

    # 1. ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸ ì„í¬íŠ¸
    from sklearn.ensemble import RandomForestClassifier
    import json # JSON ì €ì¥ì„ ìœ„í•´ import

    # 2. ëª¨ë¸ í•™ìŠµ
    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_classifier.fit(X_preprocessed, kmeans.labels_)

    # 3. ë³€ìˆ˜ ì¤‘ìš”ë„ ì¶”ì¶œ
    importances = rf_classifier.feature_importances_

    # 4. ì¤‘ìš”ë„ë¥¼ ì›ë˜ ë³€ìˆ˜ëª…ê³¼ ë§¤í•‘
    feature_names = preprocessor.get_feature_names_out()
    feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})

    # 5. ì¤‘ìš”ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì „ì²´ ë³€ìˆ˜ ì •ë ¬
    ranked_features_df = feature_importance_df.sort_values(by='importance', ascending=False)

    print("í´ëŸ¬ìŠ¤í„°ë§ì— ì¤‘ìš”í•œ ë³€ìˆ˜ ì „ì²´ ìˆœìœ„ (ìƒìœ„ 10ê°œ):")
    print(ranked_features_df.head(10))

    # 6. ì „ì²´ ìˆœìœ„ê°€ ë‹´ê¸´ DataFrameì„ CSV íŒŒì¼ë¡œ ì €ì¥ (ê¸°ì¡´ ì½”ë“œ)
    ranked_features_df.to_csv('./models/feature_importance_ranking.csv', index=False, encoding='utf-8-sig')
    print(f"\nì „ì²´ ë³€ìˆ˜ ì¤‘ìš”ë„ ìˆœìœ„ë¥¼ './models/feature_importance_ranking.csv'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")


    # -------------------- ì¤‘ìš”ë„ ì „ì²˜ë¦¬ ë° ê°€ì¤‘ì¹˜ íŒŒì¼ ìƒì„± (ì¶”ê°€ëœ ë¶€ë¶„) --------------------
    print("\nì €ì¥ëœ ì¤‘ìš”ë„ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 7. ì›ë³¸ ë³€ìˆ˜ëª… ì¶”ì¶œì„ ìœ„í•œ ìˆ˜ì •ëœ í•¨ìˆ˜
    def get_base_feature(feature_name: str, categorical_cols: list) -> str:
        """
        ì „ì²´ ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ì›ë³¸ ë³€ìˆ˜ëª…ì„ ì°¾ìŠµë‹ˆë‹¤.
        """
        # 'cat__', 'num__' ì ‘ë‘ì‚¬ ì œê±°
        clean_name = feature_name.split('__')[1]
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡ì„ í™•ì¸í•˜ì—¬ ì¼ì¹˜í•˜ëŠ” ì›ë³¸ ë³€ìˆ˜ëª… ë°˜í™˜
        for col in categorical_cols:
            if clean_name.startswith(col + '_'):
                return col
                
        # ë²”ì£¼í˜•ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return clean_name

    # ì „ì—­ì— ì •ì˜ëœ categorical_cols ë¦¬ìŠ¤íŠ¸ë¥¼ í•¨ìˆ˜ì— ì „ë‹¬

    ranked_features_df['base_feature'] = ranked_features_df['feature'].apply(
        lambda x: get_base_feature(x, categorical_cols)
    )

    # 8. ì›ë³¸ ë³€ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ í•©ì‚° ë° ì •ê·œí™”
    aggregated_importances = ranked_features_df.groupby('base_feature')['importance'].sum()

    # 8a. ì œì™¸í•  ë³€ìˆ˜ ëª©ë¡ ì •ì˜
    EXCLUDE_VARS = ['gender', 'age', 'country']
    print(f"\nì œì™¸í•  ë³€ìˆ˜: {EXCLUDE_VARS}")

    # 8b. í•´ë‹¹ ë³€ìˆ˜ë“¤ì„ ì¤‘ìš”ë„ ëª©ë¡ì—ì„œ ì œê±°
    filtered_importances = aggregated_importances.drop(labels=EXCLUDE_VARS, errors='ignore')
    print("ë³€ìˆ˜ ì œì™¸ ì™„ë£Œ.")

    # 8c. ë‚¨ì€ ë³€ìˆ˜ë“¤ì˜ ì¤‘ìš”ë„ ì´í•©ì´ 1.0ì´ ë˜ë„ë¡ ì¬ì •ê·œí™”
    renormalized_weights = filtered_importances / filtered_importances.sum()
    final_weights = renormalized_weights.sort_values(ascending=False)
    print("ë‚¨ì€ ë³€ìˆ˜ë“¤ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¬ì¡°ì •í–ˆìŠµë‹ˆë‹¤.")

    print("\nì›ë³¸ ë³€ìˆ˜ë³„ ìµœì¢… ê°€ì¤‘ì¹˜:")
    print(final_weights)

    # 9. ìµœì¢… ê°€ì¤‘ì¹˜ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ì˜ˆì¸¡ ì‹œ ì‚¬ìš©
    final_weights.to_json('./models/variable_weights.json', orient='index', indent=4)
    print(f"\nìµœì¢… ë³€ìˆ˜ ê°€ì¤‘ì¹˜ë¥¼ './models/  .json'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    # ------------------------------------------------------------------------------------


    print("\ní•™ìŠµëœ ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ './models/' í´ë”ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    print("ì´ì œ predict_app.py íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì‹¤ì‹œê°„ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()