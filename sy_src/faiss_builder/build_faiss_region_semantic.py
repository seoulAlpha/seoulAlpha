import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import os
os.environ['HF_HOME'] = 'D:/huggingface_cache'

# --- ì„¤ì • ---
# ê¸°ì¡´ ì¥ì†Œ ì„ë² ë”©ê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©
MODEL_NAME = 'jhgan/ko-sbert-nli'

# ì›ë³¸ ë°ì´í„° ê²½ë¡œ
INPUT_METADATA_FILE = 'data/faiss/faiss_merged_output/merged_metadata.jsonl'

# ìƒì„±ë  ê²°ê³¼ë¬¼ ê²½ë¡œ
OUTPUT_DIR = 'data/faiss/region_db'
OUTPUT_INDEX_FILE = f'{OUTPUT_DIR}/faiss_region_semantic.index'
OUTPUT_METADATA_FILE = f'{OUTPUT_DIR}/metadata_region_semantic.jsonl'

# --- ì„¤ì • ë ---

import os
os.makedirs(OUTPUT_DIR, exist_ok=True)


def create_region_documents():
    """ì›ë³¸ ë©”íƒ€ë°ì´í„°ì—ì„œ ì§€ì—­ë³„ë¡œ ë¬¸ì„œë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤."""
    print("1. ì§€ì—­ë³„ ëŒ€í‘œ ë¬¸ì„œ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    region_docs = {}
    
    with open(INPUT_METADATA_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            meta = json.loads(line)
            region_name = meta.get('region_l2')
            
            if not region_name:
                continue
            
            # ì§€ì—­ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            if region_name not in region_docs:
                region_docs[region_name] = []
            
            # ai_summaryì™€ processed_sentences í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€
            if meta.get('ai_summary'):
                region_docs[region_name].append(meta['ai_summary'])
            if meta.get('processed_sentences'):
                region_docs[region_name].extend(meta['processed_sentences'])
                
    print(f"âœ… ì´ {len(region_docs)}ê°œ ì§€ì—­ì˜ ëŒ€í‘œ ë¬¸ì„œë¥¼ ê·¸ë£¹í™”í–ˆìŠµë‹ˆë‹¤.")
    return region_docs


def main():
    region_docs = create_region_documents()
    
    print("\n2. ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    model = SentenceTransformer(MODEL_NAME)
    
    print("\n3. ê° ì§€ì—­ ëŒ€í‘œ ë¬¸ì„œë¥¼ ì„ë² ë”©í•©ë‹ˆë‹¤...")
    region_names = []
    region_vectors = []
    
    for region_name, texts in region_docs.items():
        # í•œ ì§€ì—­ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ í° ë¬¸ìì—´ë¡œ í•©ì¹¨
        full_document = ". ".join(texts)
        if not full_document:
            continue
        
        print(f"  - '{region_name}' ì„ë² ë”© ì¤‘...")
        vector = model.encode(full_document)
        
        region_names.append(region_name)
        region_vectors.append(vector)
    
    # ë¦¬ìŠ¤íŠ¸ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    region_vectors = np.array(region_vectors).astype('float32')
    
    # FAISS ì¸ë±ìŠ¤ ìƒì„± ë° ë²¡í„° ì¶”ê°€
    print("\n4. FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...")
    dimension = region_vectors.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(region_vectors)
    
    faiss.write_index(index, OUTPUT_INDEX_FILE)
    print(f"  - FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {OUTPUT_INDEX_FILE}")
    
    # ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥
    print("\n5. ì§€ì—­ ë©”íƒ€ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤...")
    with open(OUTPUT_METADATA_FILE, 'w', encoding='utf-8') as f:
        for i, region_name in enumerate(region_names):
            # ì¸ë±ìŠ¤ ìˆœì„œ(i)ê°€ ê³§ ë²¡í„° IDê°€ ë¨
            meta = {'region_id': i, 'region_name': region_name}
            f.write(json.dumps(meta, ensure_ascii=False) + '\n')
            
    print(f"  - ì§€ì—­ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {OUTPUT_METADATA_FILE}")
    print("\nğŸ‰ ì˜ë¯¸ ê¸°ë°˜ 'ìœ ì‚¬ ì§€ì—­' DB ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == '__main__':
    main()