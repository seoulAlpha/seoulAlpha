import os
os.environ['HF_HOME'] = 'D:/huggingface_cache'

import json
import faiss
import numpy as np

# --- ì„¤ì • ---
# ì›ë³¸ ë°ì´í„°ê°€ ìˆëŠ” ë£¨íŠ¸ í´ë” ì´ë¦„
ROOT_DIR = 'faiss_integrated_dbs'
# ê²°ê³¼ë¬¼ì„ ì €ì¥í•  í´ë” ì´ë¦„
OUTPUT_DIR = 'faiss_merged_output'
# ìƒì„±ë  í†µí•© íŒŒì¼ ì´ë¦„
MERGED_INDEX_FILE = 'merged.index'
MERGED_METADATA_FILE = 'merged_metadata.jsonl'
# --- ì„¤ì • ë ---


def preprocess_and_merge():
    """
    ì§€ì •ëœ í´ë” êµ¬ì¡°ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì—¬ëŸ¬ FAISS indexì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ì„
    í•˜ë‚˜ì˜ indexì™€ ë©”íƒ€ë°ì´í„° íŒŒì¼ë¡œ í†µí•©í•©ë‹ˆë‹¤.
    """
    # ê²°ê³¼ë¬¼ ì €ì¥ í´ë” ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    merged_index = None
    all_metadata = []
    global_vector_id_counter = 0
    is_first_index = True

    print(f"ğŸš€ í†µí•© ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ëŒ€ìƒ í´ë”: '{ROOT_DIR}'")

    # ë£¨íŠ¸ í´ë”ë¶€í„° ëª¨ë“  í•˜ìœ„ í´ë”ë¥¼ ìˆœíšŒ
    for dirpath, _, filenames in os.walk(ROOT_DIR):
        # í˜„ì¬ í´ë”ì— .index íŒŒì¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
        if not any(fname.endswith('.index') for fname in filenames):
            continue

        # í´ë” ì´ë¦„ì„ 'ì§€ì—­' ì •ë³´ë¡œ ì‚¬ìš©
        region = os.path.basename(dirpath)
        print(f"\nğŸ“‚ ì§€ì—­ [{region}] ì²˜ë¦¬ ì¤‘...")

        # ë©”íƒ€ë°ì´í„° íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ index íŒŒì¼ì„ ì°¾ìŒ
        for filename in sorted(filenames):
            if filename.startswith('metadata_') and filename.endswith('.jsonl'):
                
                # íŒŒì¼ëª…ì—ì„œ 'ì¹´í…Œê³ ë¦¬' ì •ë³´ ì¶”ì¶œ
                category = filename.replace('metadata_', '').replace('.jsonl', '')
                
                metadata_path = os.path.join(dirpath, filename)
                index_path = os.path.join(dirpath, f'faiss_{category}.index')

                if not os.path.exists(index_path):
                    print(f"  - âš ï¸ ê²½ê³ : '{metadata_path}'ì— ëŒ€í•œ ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                print(f"  - ì¹´í…Œê³ ë¦¬ '{category}' ì²˜ë¦¬ ì¤‘...")

                # ê°œë³„ FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                sub_index = faiss.read_index(index_path)

                # ì²« ë²ˆì§¸ ì¸ë±ìŠ¤ íŒŒì¼ì„ ê¸°ì¤€ìœ¼ë¡œ í†µí•© ì¸ë±ìŠ¤ì˜ ì°¨ì›ì„ ê²°ì •í•˜ê³  ì´ˆê¸°í™”
                if is_first_index:
                    dimension = sub_index.d
                    print(f"  - ë²¡í„° ì°¨ì›({dimension}) í™•ì¸. í†µí•© ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                    # ê° ë²¡í„°ì— ê³ ìœ  IDë¥¼ ë§¤í•‘í•  ìˆ˜ ìˆëŠ” IndexIDMap ì‚¬ìš©
                    index_flat = faiss.IndexFlatL2(dimension)
                    merged_index = faiss.IndexIDMap(index_flat)
                    is_first_index = False
                
                # ì°¨ì›ì´ ë§ì§€ ì•ŠëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬
                if sub_index.d != merged_index.d:
                    print(f"  - âŒ ì—ëŸ¬: ë²¡í„° ì°¨ì›ì´ ë‹¤ë¦…ë‹ˆë‹¤! (ê¸°ëŒ€: {merged_index.d}, í˜„ì¬: {sub_index.d}). ì´ íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                # ì¸ë±ìŠ¤ì—ì„œ ë²¡í„°ë“¤ê³¼ ì´ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
                vectors = sub_index.reconstruct_n(0, sub_index.ntotal)
                num_vectors = sub_index.ntotal

                # í†µí•© ì¸ë±ìŠ¤ì— ì‚¬ìš©í•  ìƒˆë¡œìš´ ê³ ìœ  ID ë°°ì—´ ìƒì„±
                new_global_ids = np.arange(global_vector_id_counter, global_vector_id_counter + num_vectors)

                # í†µí•© ì¸ë±ìŠ¤ì— ë²¡í„°ì™€ ìƒˆë¡œìš´ ID ì¶”ê°€
                merged_index.add_with_ids(vectors, new_global_ids)

                # ë©”íƒ€ë°ì´í„° ì²˜ë¦¬: í•œ ì¤„ì”© ì½ì–´ ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        try:
                            meta_obj = json.loads(line)
                            # ìƒˆë¡œìš´ ê³ ìœ  vector_id, ì§€ì—­, ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                            meta_obj['vector_id'] = int(new_global_ids[i])
                            meta_obj['region'] = region
                            meta_obj['category'] = category
                            all_metadata.append(meta_obj)
                        except (json.JSONDecodeError, IndexError) as e:
                            print(f"    - âš ï¸ ê²½ê³ : '{metadata_path}' íŒŒì¼ì˜ {i+1}ë²ˆì§¸ ì¤„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê±´ë„ˆëœë‹ˆë‹¤. ({e})")
                
                print(f"    - {num_vectors}ê°œ ë²¡í„° ì¶”ê°€ ì™„ë£Œ. (ì´ {merged_index.ntotal}ê°œ)")
                # ë‹¤ìŒ íŒŒì¼ì„ ìœ„í•´ ID ì¹´ìš´í„° ì—…ë°ì´íŠ¸
                global_vector_id_counter += num_vectors

    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ëë‚œ í›„, ìµœì¢… ê²°ê³¼ë¬¼ ì €ì¥
    if merged_index and len(all_metadata) > 0:
        output_index_path = os.path.join(OUTPUT_DIR, MERGED_INDEX_FILE)
        output_metadata_path = os.path.join(OUTPUT_DIR, MERGED_METADATA_FILE)

        print("\nğŸ’¾ ìµœì¢… íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤...")
        faiss.write_index(merged_index, output_index_path)
        print(f"  - í†µí•© ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {output_index_path}")

        with open(output_metadata_path, 'w', encoding='utf-8') as f:
            for meta_obj in all_metadata:
                # ensure_ascii=False ì˜µì…˜ìœ¼ë¡œ í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ ì €ì¥
                f.write(json.dumps(meta_obj, ensure_ascii=False) + '\n')
        print(f"  - í†µí•© ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_metadata_path}")
        
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"  - ì´ {merged_index.ntotal}ê°œì˜ ë²¡í„°ê°€ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'ROOT_DIR' ì„¤ì •ê³¼ íŒŒì¼ êµ¬ì¡°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")


if __name__ == '__main__':
    preprocess_and_merge()