import time
import json
import os
from tqdm import tqdm  # [ì¶”ê°€] tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options

# ===================================================================
# 1. ìŠ¤í¬ë˜í•‘í•  ë°ì´í„° ëª©ë¡ ì •ì˜
# ===================================================================

# ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬
CATEGORIES = ['ì¹´í˜', 'ì‹ë‹¹', 'ê´€ê´‘ëª…ì†Œ', 'ë¬¸í™”ì¬', 'ë ˆì €', 'ì‡¼í•‘', 'ìˆ™ë°•']

# í–‰ì •êµ¬ì—­ ì •ë³´ (ì „ì²´ ëª©ë¡ì€ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ ì˜ˆì‹œë¡œ í¬í•¨)
# ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ì´ì „ì— ì œê³µëœ ì „ì²´ ëª©ë¡ì„ ë³µì‚¬í•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.
ADMIN_AREAS = {
    # Batch1
    "ì„œìš¸íŠ¹ë³„ì‹œ": ["ê°•ë‚¨êµ¬", "ê°•ë™êµ¬", "ê°•ë¶êµ¬", "ê°•ì„œêµ¬", "ê´€ì•…êµ¬", "ê´‘ì§„êµ¬", "êµ¬ë¡œêµ¬", "ê¸ˆì²œêµ¬", "ë…¸ì›êµ¬", "ë„ë´‰êµ¬", "ë™ëŒ€ë¬¸êµ¬", "ë™ì‘êµ¬", "ë§ˆí¬êµ¬", "ì„œëŒ€ë¬¸êµ¬", "ì„œì´ˆêµ¬", "ì„±ë™êµ¬", "ì„±ë¶êµ¬", "ì†¡íŒŒêµ¬", "ì–‘ì²œêµ¬", "ì˜ë“±í¬êµ¬", "ìš©ì‚°êµ¬", "ì€í‰êµ¬", "ì¢…ë¡œêµ¬", "ì¤‘êµ¬", "ì¤‘ë‘êµ¬"],
    "ë¶€ì‚°ê´‘ì—­ì‹œ": ["ê°•ì„œêµ¬", "ê¸ˆì •êµ¬", "ê¸°ì¥êµ°", "ë‚¨êµ¬", "ë™êµ¬", "ë™ë˜êµ¬", "ë¶€ì‚°ì§„êµ¬", "ë¶êµ¬", "ì‚¬ìƒêµ¬", "ì‚¬í•˜êµ¬", "ì„œêµ¬", "ìˆ˜ì˜êµ¬", "ì—°ì œêµ¬", "ì˜ë„êµ¬", "ì¤‘êµ¬", "í•´ìš´ëŒ€êµ¬"],
    "ëŒ€êµ¬ê´‘ì—­ì‹œ": ["êµ°ìœ„êµ°", "ë‚¨êµ¬", "ë‹¬ì„œêµ¬", "ë‹¬ì„±êµ°", "ë™êµ¬", "ë¶êµ¬", "ì„œêµ¬", "ìˆ˜ì„±êµ¬", "ì¤‘êµ¬"],
    "ì¸ì²œê´‘ì—­ì‹œ": ["ê°•í™”êµ°", "ê³„ì–‘êµ¬", "ë‚¨ë™êµ¬", "ë™êµ¬", "ë¯¸ì¶”í™€êµ¬", "ë¶€í‰êµ¬", "ì„œêµ¬", "ì—°ìˆ˜êµ¬", "ì˜¹ì§„êµ°", "ì¤‘êµ¬"]
    

    # Batch2
    #"ê´‘ì£¼ê´‘ì—­ì‹œ": ["ê´‘ì‚°êµ¬", "ë‚¨êµ¬", "ë™êµ¬", "ë¶êµ¬", "ì„œêµ¬"],
    #"ëŒ€ì „ê´‘ì—­ì‹œ": ["ëŒ€ë•êµ¬", "ë™êµ¬", "ì„œêµ¬", "ìœ ì„±êµ¬", "ì¤‘êµ¬"],
    # "ìš¸ì‚°ê´‘ì—­ì‹œ": ["ë‚¨êµ¬", "ë™êµ¬", "ë¶êµ¬", "ìš¸ì£¼êµ°", "ì¤‘êµ¬"],
    #"ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": ["ì„¸ì¢…ì‹œ"],
    # "ê²½ê¸°ë„": ["ìˆ˜ì›ì‹œ", "ìš©ì¸ì‹œ", "ê³ ì–‘ì‹œ", "ì„±ë‚¨ì‹œ", "í™”ì„±ì‹œ", "ë¶€ì²œì‹œ", "ë‚¨ì–‘ì£¼ì‹œ", "ì•ˆì‚°ì‹œ", "í‰íƒì‹œ", "ì•ˆì–‘ì‹œ", "ì‹œí¥ì‹œ", "íŒŒì£¼ì‹œ", "ê¹€í¬ì‹œ", "ì˜ì •ë¶€ì‹œ", "ê´‘ì£¼ì‹œ", "í•˜ë‚¨ì‹œ", "ì˜¤ì‚°ì‹œ", "ì´ì²œì‹œ", "ì•ˆì„±ì‹œ", "ì˜ì™•ì‹œ", "ì–‘ì£¼ì‹œ", "êµ¬ë¦¬ì‹œ", "í¬ì²œì‹œ", "ë™ë‘ì²œì‹œ", "ê³¼ì²œì‹œ", "ì—¬ì£¼ì‹œ", "ì–‘í‰êµ°", "ê°€í‰êµ°", "ì—°ì²œêµ°"],
    # "ê°•ì›íŠ¹ë³„ìì¹˜ë„": ["ì¶˜ì²œì‹œ", "ì›ì£¼ì‹œ", "ê°•ë¦‰ì‹œ", "ë™í•´ì‹œ", "íƒœë°±ì‹œ", "ì†ì´ˆì‹œ", "ì‚¼ì²™ì‹œ", "í™ì²œêµ°", "íš¡ì„±êµ°", "ì˜ì›”êµ°", "í‰ì°½êµ°", "ì •ì„ êµ°", "ì² ì›êµ°", "í™”ì²œêµ°", "ì–‘êµ¬êµ°", "ì¸ì œêµ°", "ê³ ì„±êµ°", "ì–‘ì–‘êµ°"],

    # Batch3
    # "ì¶©ì²­ë¶ë„": ["ì²­ì£¼ì‹œ", "ì¶©ì£¼ì‹œ", "ì œì²œì‹œ", "ë³´ì€êµ°", "ì˜¥ì²œêµ°", "ì˜ë™êµ°", "ì¦í‰êµ°", "ì§„ì²œêµ°", "ê´´ì‚°êµ°", "ìŒì„±êµ°", "ë‹¨ì–‘êµ°"],
    # "ì¶©ì²­ë‚¨ë„": ["ì²œì•ˆì‹œ", "ê³µì£¼ì‹œ", "ë³´ë ¹ì‹œ", "ì•„ì‚°ì‹œ", "ì„œì‚°ì‹œ", "ë…¼ì‚°ì‹œ", "ê³„ë£¡ì‹œ", "ë‹¹ì§„ì‹œ", "ê¸ˆì‚°êµ°", "ë¶€ì—¬êµ°", "ì„œì²œêµ°", "ì²­ì–‘êµ°", "í™ì„±êµ°", "ì˜ˆì‚°êµ°", "íƒœì•ˆêµ°"],
    # "ì „ë¶íŠ¹ë³„ìì¹˜ë„": ["ì „ì£¼ì‹œ", "ìµì‚°ì‹œ", "êµ°ì‚°ì‹œ", "ì •ìì‹œ", "ë‚¨ì›ì‹œ", "ê¹€ì œì‹œ", "ì™„ì£¼êµ°", "ì§„ì•ˆêµ°", "ë¬´ì£¼êµ°", "ì¥ìˆ˜êµ°", "ì„ì‹¤êµ°", "ìˆœì°½êµ°", "ê³ ì°½êµ°", "ë¶€ì•ˆêµ°"],
    # "ì „ë¼ë‚¨ë„": ["ëª©í¬ì‹œ", "ì—¬ìˆ˜ì‹œ", "ìˆœì²œì‹œ", "ë‚˜ì£¼ì‹œ", "ê´‘ì–‘ì‹œ", "ë‹´ì–‘êµ°", "ê³¡ì„±êµ°", "êµ¬ë¡€êµ°", "ê³ í¥êµ°", "ë³´ì„±êµ°", "í™”ìˆœêµ°", "ì¥í¥êµ°", "ê°•ì§„êµ°", "í•´ë‚¨êµ°", "ì˜ì•”êµ°", "ë¬´ì•ˆêµ°", "í•¨í‰êµ°", "ì˜ê´‘êµ°", "ì¥ì„±êµ°", "ì™„ë„êµ°", "ì§„ë„êµ°", "ì‹ ì•ˆêµ°"],
    # "ê²½ìƒë¶ë„": ["í¬í•­ì‹œ", "ê²½ì£¼ì‹œ", "ê¹€ì²œì‹œ", "ì•ˆë™ì‹œ", "êµ¬ë¯¸ì‹œ", "ì˜ì£¼ì‹œ", "ì˜ì²œì‹œ", "ìƒì£¼ì‹œ", "ë¬¸ê²½ì‹œ", "ê²½ì‚°ì‹œ", "ì˜ì„±êµ°", "ì²­ì†¡êµ°", "ì˜ì–‘êµ°", "ì˜ë•êµ°", "ì²­ë„êµ°", "ê³ ë ¹êµ°", "ì„±ì£¼êµ°", "ì¹ ê³¡êµ°", "ì˜ˆì²œêµ°", "ë´‰í™”êµ°", "ìš¸ì§„êµ°", "ìš¸ë¦‰êµ°"],
    # "ê²½ìƒë‚¨ë„": ["ì°½ì›ì‹œ", "ì§„ì£¼ì‹œ", "í†µì˜ì‹œ", "ì‚¬ì²œì‹œ", "ê¹€í•´ì‹œ", "ë°€ì–‘ì‹œ", "ê±°ì œì‹œ", "ì–‘ì‚°ì‹œ", "ì˜ë ¹êµ°", "í•¨ì•ˆêµ°", "ì°½ë…•êµ°", "ê³ ì„±êµ°", "ë‚¨í•´êµ°", "í•˜ë™êµ°", "ì‚°ì²­êµ°", "í•¨ì–‘êµ°", "ê±°ì°½êµ°", "í•©ì²œêµ°"],
    # "ì œì£¼íŠ¹ë³„ìì¹˜ë„": ["ì œì£¼ì‹œ", "ì„œê·€í¬ì‹œ"]
}

# ===================================================================
# 2. í•µì‹¬ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ ì •ì˜
# ===================================================================
def scrape_query(query):
    """
    ì£¼ì–´ì§„ ê²€ìƒ‰ì–´(query)ì— ëŒ€í•´ ì¹´ì¹´ì˜¤ë§µì„ ìŠ¤í¬ë˜í•‘í•˜ê³  ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    
    # [ìˆ˜ì •] ë°ì´í„° ì €ì¥ í´ë” ê²½ë¡œ ë³€ê²½
    province = query.split("_")[0]
    province, file_name = query.split(" ", 1)
    file_name = file_name.replace(" ", "_")
    output_dir = f'data/json/{province}'
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    output_filename = os.path.join(output_dir, f"{file_name}.jsonl")


    # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ê±´ë„ˆë›°ê¸°
    if os.path.exists(output_filename):
        return # tqdm ì§„í–‰ë¥ ì„ ìœ„í•´ printë¬¸ì€ ì œê±°

    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument('--window-size=1920x1080')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")
    
    driver = webdriver.Chrome(options=options)
    
    try:
        driver.get("https://map.kakao.com/")
        wait = WebDriverWait(driver, 10)
        
        search_box = wait.until(EC.presence_of_element_located((By.ID, "search.keyword.query")))
        search_box.send_keys(query)
        search_box.send_keys(Keys.ENTER)
        wait.until(EC.presence_of_element_located((By.CLASS_NAME, "placelist")))
        time.sleep(2)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        place_items = soup.select('ul.placelist > li.PlaceItem')
        
        places_to_scrape = []
        for item in place_items:
            try:
                place_name = item.select_one('a.link_name').get_text(strip=True)
                comment_link_tag = item.select_one('a[href*="#comment"]')
                if comment_link_tag:
                    places_to_scrape.append({
                        "name": place_name,
                        "address": item.select_one('div.addr > p:nth-of-type(1)').get_text(strip=True),
                        "url": comment_link_tag['href'].split('#')[0]
                    })
            except AttributeError:
                continue
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•¨ìˆ˜ ì¢…ë£Œ
        if not places_to_scrape:
            return

        all_places_details = []
        for place in places_to_scrape:
            driver.get(place['url'] + '#comment')
            time.sleep(2)
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            raw_comments = [p.get_text(strip=True) for p in soup.select('p.desc_review')]
            comments = []
            for text in raw_comments:
                if '...ë”ë³´ê¸°' in text:
                    end_marker_pos = -1
                    for marker in ['.', '!', '?', 'ìš”']:
                        pos = text.rfind(marker, 0, text.find('...ë”ë³´ê¸°'))
                        if pos > end_marker_pos: end_marker_pos = pos
                    comments.append(text[:end_marker_pos + 1] if end_marker_pos != -1 else text.split('...ë”ë³´ê¸°')[0])
                else:
                    comments.append(text)
            
            driver.get(place['url'] + '#review')
            time.sleep(2)
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            ai_summary_div = soup.select_one('div.ai_summary')
            ai_summary_text = ", ".join([span.get_text(strip=True) for span in ai_summary_div.select('span.option_review')]) if ai_summary_div else ""
            
            all_places_details.append({
                "name": place['name'], "address": place['address'], 
                "comments": comments, "ai_summary": ai_summary_text
            })

        #with open(output_filename, 'w', encoding='utf-8') as f:
        #    json.dump(all_places_details, f, ensure_ascii=False, indent=4)

        with open(output_filename, 'w', encoding='utf-8') as f:
            # all_places_details ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©(ë”•ì…”ë„ˆë¦¬)ì„ ìˆœíšŒí•©ë‹ˆë‹¤.
            for place_detail in all_places_details:
                # ê° ë”•ì…”ë„ˆë¦¬ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                json_line = json.dumps(place_detail, ensure_ascii=False)
                # ë³€í™˜ëœ ë¬¸ìì—´ í•œ ì¤„ê³¼ ì¤„ë°”ê¿ˆ ë¬¸ì(\n)ë¥¼ íŒŒì¼ì— ì”ë‹ˆë‹¤.
                f.write(json_line + '\n')

    except Exception as e:
        # tqdm ì§„í–‰ë¥  í‘œì‹œì¤„ê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ ì—ëŸ¬ ë©”ì‹œì§€ í˜•ì‹ ë³€ê²½
        tqdm.write(f"*** ì—ëŸ¬ ë°œìƒ: [{query}] | ì‚¬ìœ : {e} ***")
    
    finally:
        driver.quit()

# ===================================================================
# 3. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡
# ===================================================================
if __name__ == "__main__":
    # [ì¶”ê°€] tqdmì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì „ì²´ ì‘ì—… ëª©ë¡ì„ ë¯¸ë¦¬ ìƒì„±
    all_queries = []
    for province, cities in ADMIN_AREAS.items():
        for city in cities:
            for category in CATEGORIES:
                province_short = province.replace("íŠ¹ë³„ì‹œ", "").replace("ê´‘ì—­ì‹œ", "").replace("íŠ¹ë³„ìì¹˜ë„", "").replace("íŠ¹ë³„ìì¹˜ì‹œ", "")
                search_query = f"{province_short} {city} {category}"
                all_queries.append(search_query)

    print(f"ì´ {len(all_queries)}ê°œì˜ ê²€ìƒ‰ì–´ì— ëŒ€í•œ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # [ìˆ˜ì •] tqdmì„ ì ìš©í•˜ì—¬ ë°˜ë³µ ì‹¤í–‰
    for query in tqdm(all_queries, desc="ì „êµ­ ìŠ¤í¬ë˜í•‘ ì§„í–‰ë¥ "):
        scrape_query(query)
        # ì¹´ì¹´ì˜¤ë§µì˜ ê³¼ë„í•œ ìš”ì²­ ë°©ì§€ë¥¼ ìœ„í•œ íœ´ì‹ ì‹œê°„
        time.sleep(5) # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì´ì „ë³´ë‹¤ ì‹œê°„ì„ ì•½ê°„ ì¤„ì„

    print("\n\nğŸ‰ğŸ‰ğŸ‰ ëª¨ë“  ìŠ¤í¬ë˜í•‘ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ğŸ‰ğŸ‰ğŸ‰")